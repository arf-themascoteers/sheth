{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable feature selection (DFS) demo\n",
    "\n",
    "Performs feature subset selection on a dataset and then evaluates those features with a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import yaml\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# the following helper class is adapted from \n",
    "# https://stackoverflow.com/questions/4417546/constantly-print-subprocess-output-while-process-is-running\n",
    "class Runner():\n",
    "    \n",
    "    def __init__(self, cmd, print=True):\n",
    "        self.cmd = cmd\n",
    "        self.print = print\n",
    "        self.stdout = ''\n",
    "        if self.print:\n",
    "            stderr = subprocess.STDOUT\n",
    "        else:\n",
    "            stderr = subprocess.PIPE\n",
    "            self.stderr = ''\n",
    "        self.proc = subprocess.Popen(self.cmd.split(), stdout=subprocess.PIPE,\n",
    "                                     stderr=stderr, universal_newlines=True)\n",
    "        self.returncode = None\n",
    "        \n",
    "    def __call__(self):\n",
    "        for stdout_line in iter(self.proc.stdout.readline, ''):\n",
    "            yield stdout_line \n",
    "        self.proc.stdout.close()\n",
    "        self.returncode = self.proc.wait()\n",
    "        \n",
    "    def run(self):\n",
    "        if self.print:\n",
    "            for l in self():\n",
    "                if self.print:\n",
    "                    print(l, end='')\n",
    "                self.stdout += l\n",
    "        else:\n",
    "            self.stdout,self.stderr = self.proc.communicate()\n",
    "            self.returncode = self.proc.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform subset selection w/ DFS on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "usage: dfs.py [-h] [--dataset_config PATH] [--dn_data DIR] [--device DEVICE]\n",
    "              [--order ORDER] [--penalty PENALTY] [--lr LR] [--epochs EPOCHS]\n",
    "              [--workers WORKERS] [--seed SEED] [--batch BATCH]\n",
    "              [--path_output PATH]\n",
    "              dataset\n",
    "              \n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --dataset_config PATH\n",
    "                        yml file holding values for fn_train, fn_eval, ncols,\n",
    "                        nrows, nrows_test, zero_based, neg_label, binary\n",
    "                        (default: ./datasets.yml)\n",
    "  --dn_data DIR         location of train/test files; mappings/datastats files\n",
    "                        stored here (default: .)\n",
    "  --device DEVICE       (default: 'cuda' if available, else 'cpu')\n",
    "  --order ORDER         {1..12} (default: 4)\n",
    "  --penalty PENALTY     (0,infty) (default: 10)\n",
    "  --lr LR               Adam learning rate (default: 0.1)\n",
    "  --epochs EPOCHS       (default: 1.0)\n",
    "  --workers WORKERS     for the pytorch dataloader (default: 4)\n",
    "  --seed SEED           pytorch seed (default: 0)\n",
    "  --batch BATCH         target batchsize (default: 1000)\n",
    "  --path_output PATH    output text file with selected features (default:\n",
    "                        ./dfs.features.NUM_SELECTED_FEATURES.txt)\n",
    "\"\"\"\n",
    "\n",
    "# the urls for the train/test files are provided in the README\n",
    "dataset = 'rcv1'\n",
    "dn_data = os.path.expanduser('~/dfs_data')\n",
    "\n",
    "order = 4\n",
    "w_penalty = 2e1\n",
    "path_output = '%s.dfs.%d.%g.features.txt' % (dataset,order,w_penalty)\n",
    "\n",
    "cmd = ('python dfs.py %s --dn_data %s --penalty %g --path_output %s'%(dataset,dn_data,w_penalty,path_output))\n",
    "\n",
    "\"\"\"\n",
    "for the first run on a dataset, DFS:\n",
    "    locates newlines for the train file,\n",
    "    estimates means/standard deviations,\n",
    "    estimates spectral norm iteratively (10 iters).\n",
    "the estimates are based on the first 10000 examples/labels.\n",
    "the newline mappings and dataset statistics are then saved (in dn_data) and loaded for subsequent runs.\n",
    "\"\"\"\n",
    "\n",
    "runner = Runner(cmd)\n",
    "runner.run()\n",
    "assert not runner.returncode, 'Failed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a linear model on train with selected features using MISSION's SGD. Then, predict on test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# NOTE: mission_logistic_eval in MISSION/src/ needs to be compiled first\n",
    "\n",
    "mission_eval_exec = 'MISSION/src/mission_logistic_eval'\n",
    "datasets = yaml.safe_load(open('datasets.yml','rt'))\n",
    "\n",
    "cmd = ('%s %s %s %s %d'%(mission_eval_exec,\n",
    "                         os.path.join(dn_data,datasets[dataset]['fn_train']),\n",
    "                         os.path.join(dn_data,datasets[dataset]['fn_eval']),\n",
    "                         path_output, datasets[dataset]['neg_label']))\n",
    "\n",
    "runner = Runner(cmd, print=False)\n",
    "runner.run()\n",
    "assert not runner.returncode, 'Failed: %s'%runner.stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Y = [[float(z) for z in x.split(' ')] for x in runner.stdout.splitlines()]\n",
    "print('DFS AUC on test: %g'%roc_auc_score([y[0] for y in Y],[y[1] for y in Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare w/ MISSION feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# NOTE: mission_logistic in MISSION/src/ needs to be compiled first\n",
    "\n",
    "mission_exec = 'MISSION/src/mission_logistic'\n",
    "\n",
    "n_feats = len(open(path_output,'rt').read().strip().splitlines())\n",
    "\n",
    "cmd = ('%s %s %s %s %d'%(mission_exec,\n",
    "                         os.path.join(dn_data,datasets[dataset]['fn_train']),\n",
    "                         os.path.join(dn_data,datasets[dataset]['fn_eval']),\n",
    "                         n_feats, datasets[dataset]['neg_label']))\n",
    "\n",
    "runner = Runner(cmd, print=False)\n",
    "runner.run()\n",
    "assert not runner.returncode, 'Failed: %s'%runner.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "Y = [[float(z) for z in x.split(' ')] for x in runner.stdout.splitlines()]\n",
    "print('MISSION AUC on test: %g'%roc_auc_score([y[0] for y in Y],[y[1] for y in Y]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfs",
   "language": "python",
   "name": "dfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
